{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7YWLLhmXTjvy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0f25b10-f1bc-49bf-aa33-42d6ebeb0b95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "# Assignment 1 Josiah Galloway 101296257 \n",
        "# Importing libraries \n",
        "from keras.datasets import fashion_mnist\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import utils\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "#Had to use this as a fix to get the setting 2 to run \n",
        "# Convert the labels to one-hot encoded format\n",
        "# train_labels = to_categorical(train_labels)\n",
        "# test_labels = to_categorical(test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Class Labels for Fashion Mnist \n",
        "\n",
        "# 0.\tT-shirt/top\n",
        "# 1.\tTrouser\n",
        "# 2.\tPullover\n",
        "# 3.\tDress\n",
        "# 4.\tCoat\n",
        "# 5.\tSandal\n",
        "# 6.\tShirt\n",
        "# 7.\tSneaker\n",
        "# 8.\tBag\n",
        "# 9.\tAnkle boot\n",
        "\n"
      ],
      "metadata": {
        "id": "o0Qn7MQoUkkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. What are the dimensions of train_images, train_labels, test_images, and test_labels?\n",
        "print(\"Dimensions of train_images:\", train_images.shape)\n",
        "print(\"Dimensions of train_labels:\", train_labels.shape)\n",
        "print(\"Dimensions of test_images:\", test_images.shape)\n",
        "print(\"Dimensions of test_labels:\", test_labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6pUhw-elVEze",
        "outputId": "98f12028-6b01-4a2e-82d7-f15d30f9d430"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dimensions of train_images: (60000, 28, 28)\n",
            "Dimensions of train_labels: (60000,)\n",
            "Dimensions of test_images: (10000, 28, 28)\n",
            "Dimensions of test_labels: (10000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. What are the lengths of the train_labels and test_labels?\n",
        "\n",
        "# train_labels has a length of: 60000\n",
        "# test_labels has a length of : 10000"
      ],
      "metadata": {
        "id": "n1-F7JnMVsXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Please Show some of the train and test labels.\n",
        "\n",
        "# Showing first 5 train labels\n",
        "print(\"Training labels:\")\n",
        "print(train_labels[:5])\n",
        "\n",
        "# Showing first 5 test labels \n",
        "print(\"Test labels:\")\n",
        "print(test_labels[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54fejU1nWmXI",
        "outputId": "c6dd66c2-8723-45ad-ee01-48bb5a857be5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training labels:\n",
            "[9 0 0 3 0]\n",
            "Test labels:\n",
            "[9 2 1 1 6]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Please plot the image of the index 5 in the training dataset \n",
        "\n",
        "print(train_images[5])"
      ],
      "metadata": {
        "id": "TT8eRUy4Xmjx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba01041a-d003-4181-d7d7-97d2e1e4cb55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  0   0   0   0   1   0   0   0   0  22  88 188 172 132 125 141 199 143\n",
            "    9   0   0   0   1   0   0   0   0   0]\n",
            " [  0   0   0   1   0   0  20 131 199 206 196 202 242 255 255 250 222 197\n",
            "  206 188 126  17   0   0   0   0   0   0]\n",
            " [  0   0   0   1   0  35 214 191 183 178 175 168 150 162 159 152 158 179\n",
            "  183 189 195 185  82   0   0   0   0   0]\n",
            " [  0   0   0   0   0 170 190 172 177 176 171 169 162 155 148 154 169 174\n",
            "  175 175 177 183 188  12   0   0   0   0]\n",
            " [  0   0   0   0  25 194 180 178 174 184 187 189 187 184 181 189 200 197\n",
            "  193 190 178 175 194  90   0   0   0   0]\n",
            " [  0   0   0   0  42 218 191 197 208 204 211 209 210 212 211 214 215 213\n",
            "  214 211 211 191 200 158   0   0   0   0]\n",
            " [  0   0   0   0  88 221 215 217 219 211 185 150 118 107  99  88  83  90\n",
            "  135 212 203 207 219 169   0   0   0   0]\n",
            " [  0   0   0   0   0  27 118 162  40   0   0   0  10  19  28  39  47  36\n",
            "    0   0 203 230 220 203   0   0   0   0]\n",
            " [  0   0   0   0 138 136  71  69  54 216 217 203 184 168 163 162 163 178\n",
            "  221 186  38  26   7   0   0   0   0   0]\n",
            " [  0   0   0   0  67 134 154 224 129  66  81 117 129 128 132 137 131 129\n",
            "   86  73 157 151 134 216  18   0   0   0]\n",
            " [  0   0   0   0 203 198 172 183 206 255 255 250 243 240 239 235 238 244\n",
            "  255 238 184 160  86  98   0   0   0   0]\n",
            " [  0   0   0   0 122 188 224 151 105 127  97 100 105 114 117 117 113 103\n",
            "   98 111 142 254 191 255  49   0   0   0]\n",
            " [  0   0   0   0 163 179 200  95 154 198 197 200 200 198 197 198 199 202\n",
            "  200 176  86 206 157 162  10   0   0   0]\n",
            " [  0   0   0   0 197 201 229  71 144 194 181 183 179 182 180 179 180 190\n",
            "  185 197  76 219 185 201  34   0   0   0]\n",
            " [  0   0   0   0 199 193 226  58 154 192 184 187 184 186 184 185 183 192\n",
            "  191 200  56 219 203 207  60   0   0   0]\n",
            " [  0   0   0   0 201 194 224  41 163 190 186 186 184 185 183 185 178 190\n",
            "  194 202  33 211 200 206  73   0   0   0]\n",
            " [  0   0   0   0 201 197 222  17 172 190 186 187 182 186 185 187 180 187\n",
            "  193 202  26 212 202 203  76   0   0   0]\n",
            " [  0   0   0   0 200 197 223   0 177 189 184 185 178 184 183 184 180 183\n",
            "  189 203  35 196 203 203  84   0   0   0]\n",
            " [  0   0   0   0 200 197 223   0 185 187 185 187 180 184 182 183 178 182\n",
            "  183 205  44 159 207 201  85   0   0   0]\n",
            " [  0   0   0   0 187 198 225   0 194 188 184 185 180 183 183 184 181 181\n",
            "  177 206  46 129 211 200  88   0   0   0]\n",
            " [  0   0   0   6 186 200 211   0 199 189 184 184 185 182 183 184 185 182\n",
            "  175 205  50  97 216 197  93   0   0   0]\n",
            " [  0   0   0   5 185 204 184   0 202 188 182 182 183 183 184 182 180 182\n",
            "  174 202  63  59 220 196  94   0   0   0]\n",
            " [  0   0   0   5 184 206 157   0 204 187 187 189 192 190 190 191 190 187\n",
            "  183 202  78  35 222 197  95   0   0   0]\n",
            " [  0   0   0   5 183 208 127   0 197 166 153 149 149 146 148 149 150 151\n",
            "  158 191  90   8 223 195  99   0   0   0]\n",
            " [  0   0   0   6 184 208 114   0 204 173 161 180 176 172 173 173 174 176\n",
            "  162 202 115   0 229 199 105   0   0   0]\n",
            " [  0   0   0   9 178 204 115   0 121 135 114 117 114 114 117 118 119 117\n",
            "  113 147  63   0 225 196 107   0   0   0]\n",
            " [  0   0   0  18 180 206 131   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0 224 197 123   0   0   0]\n",
            " [  0   0   0   0 141 151  76   0   1   1   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0 133 167  73   0   0   0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5.Please plot the image of the index 5 in the training dataset.\n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "plt.imshow(train_images[5], cmap= plt.cm.binary)\n",
        "plt.show"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "Yf7M-456YdWy",
        "outputId": "9a9978d9-f0e7-4a58-9765-61c449f7d202"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function matplotlib.pyplot.show(*args, **kw)>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. What is the label for the index 5 in the train_label and looking up in the above list, what does it mean?\n",
        "\n",
        "# Get the label for index 5\n",
        "\n",
        "print(train_labels[5])\n",
        "\n",
        "# We can think of the list above as a classic python list and the value from the train_labels[5] as the index\n",
        "# Index 2 would correspond with the category 'Pullover'\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehPr3KC6ZNyC",
        "outputId": "bfba1b33-5be1-433d-dd1d-4c5de14a4db0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Please show the digital content of image index 500 in the testing dataset\n",
        "\n",
        "print(test_images[500])"
      ],
      "metadata": {
        "id": "oLIbpRtnbCpr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2023862e-8adb-45a3-ad57-4d59c2186692"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  0   0   0   0   0   0   0   0   0   0 108  93  38   0   0   0   8  76\n",
            "  140  34   0   0   0   1   1   0   0   0]\n",
            " [  0   0   0   0   0   0   0  30  92  69 192 183 149 214 177 194 227 154\n",
            "  206 143  79  46   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0  59 127 116  85  63 184 128 104   0  10 126 138\n",
            "  145  59 104 118  86   0   0   0   0   0]\n",
            " [  0   0   0   0   0   3  99  92  81  81  54  67 222 255 209 206 178 184\n",
            "   72  69  97  97 118  24   0   2   0   0]\n",
            " [  0   0   1   1   0  60 121 138  83  64  76  68  23  35  55  52  46  36\n",
            "   65  72  72  76 102 110   0   0   0   0]\n",
            " [  0   0   0   0   0 112  89  52  89  72  67  69  81  73  60  71  64  55\n",
            "   52  73  77  88 104 108  10   0   1   0]\n",
            " [  0   0   0   0  21 110 143 131  55 181 128  54  36 158 185  44  74 150\n",
            "  154  71  96  88 105 131  50   0   0   0]\n",
            " [  0   0   2   0  52 139  90  99  77 187 134 238  68 203 127  31 158 165\n",
            "  191  73  95  83  86  91  85   0   0   0]\n",
            " [  0   0   0   0  97 104 119 135  95 137 156 209 158 185  67  91 199 150\n",
            "  218 123  78 106  91 129 106   0   0   0]\n",
            " [  0   0   0   0 109  97 129 109  80 154 183 153 178 170  78 178 228 156\n",
            "  214 135 112  92  91 101 127   4   0   0]\n",
            " [  0   0   0  10 125 102 120 146  99 189 238  76 139 111  63 238 172  48\n",
            "  249  91  79  79 146 110 111  35   0   0]\n",
            " [  0   0   0  32 147  78 156 143  55 165 175  64 142 114  62 152 107  68\n",
            "  213  90 108 155 211  74 143  53   0   0]\n",
            " [  0   0   0  46 127 105 124 246  90  69  74 101 112  79  99 112  81  96\n",
            "  184  97  77 112 172  65 119 100   0   0]\n",
            " [  0   0   0  87 143 107 112 213  70 116 230  83 154 146 150 223  65  93\n",
            "  196 121  89 146 193  92 108 105   0   0]\n",
            " [  0   0   0  97 125 125 124 157  60 119 250  70 180 158 134 177 163 107\n",
            "  225  82  85 167 178  87 123 114   0   0]\n",
            " [  0   0   0 107 102 135 142 117  78 123 229  36 121 129 119 162 223 117\n",
            "  154  88  79 186 164  68 116 129  22   0]\n",
            " [  0   0  14 116 107 133 154 124  71 156 193  85 181  92 121 167 194 185\n",
            "  119  78 101 138 163 105 118 101  27   0]\n",
            " [  0   0  15 117 108 117 180 129  76 117 189 208 211 105 161 187 190 221\n",
            "   97  98 107  93 118 104 124 128  39   0]\n",
            " [  0   0  15 112  96 114 167 115 101  81 181 170 216 154 181 193 176 220\n",
            "   72 107  93 102  97 120 129 111  50   0]\n",
            " [  0   0  23 129 110 121 178  80  85 109  70  63  76 108  92  60  68  65\n",
            "  105  90 116  91 105 102 105 110  40   0]\n",
            " [  0   0  38 119 102 142 158  97 104 100 121 119 107 121 120  89  96 127\n",
            "  101  99 107  95 139 163  98 108  53   0]\n",
            " [  0   0  21 102  97 148  93  80 124  92  97  99 109 131 126 109 107 104\n",
            "   97 104  92 106  76 165 124 129  63   0]\n",
            " [  0   0  39 106 111 130  72 104  85  96  97  96 104 129 116  88  96  99\n",
            "  101  90 116 106  44 147 127 106  42   0]\n",
            " [  0   0  45 131 158  83  51 128  86 101 108 100 110 137 123  91  93 114\n",
            "  102 108 106 117  33 125 137 101  52   0]\n",
            " [  0   0  43  87 123  79  58 116 110  81 105 119 120 140 124 115 125 110\n",
            "   90 110  83 140   8  99 152 117  69   0]\n",
            " [  0   0  49 121 155  51  42 116  95  97  95  93 108 147 117  83  90  89\n",
            "  104  97  93 131   0  67 153  95  57   0]\n",
            " [  0   0  42  93 143  19  49 112  89 101 107  99 104 145 135  97 108 112\n",
            "   96  96  99 128   0  27 152  92  62   0]\n",
            " [  0   0  58  91 143  10  34  88  44  63  72  68  64  71  70  62  69  59\n",
            "   55  77  71 112   0  20 138  89  65   0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8 Please plot the image of the index 500 \n",
        "\n",
        "plt.imshow(test_images[500], cmap = plt.cm.binary)\n",
        "plt.show\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "mEH5SG6dbdWq",
        "outputId": "97f3bf46-b771-460c-d5f0-ae6228509306"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function matplotlib.pyplot.show(*args, **kw)>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAVRUlEQVR4nO3da2zVZbYG8GdZoJRyp9AW6NBSbiIims0lkSgycRA00YkRL8nEY/QwJqhDMjHHeD6MfkDNicxkEk5MmCMZZhycKDNGP+g5A40RDCItglABD1gKUnoDCpQ7tOt86NZTsf+1Ovte3ueXkLb76ct+u8tit3v93/cVVQURXf9uyPYEiCgzWOxEgWCxEwWCxU4UCBY7USD6ZfLOioqKtLy8PJN3eV1oaWkx87y8vMissLDQHDtw4MCE5tRbVrfnzJkz5tj29nYzHz16tJnn5+eb+fWovr4ex48fl56ypIpdRO4B8HsAeQD+S1Vfsz6/vLwcNTU1ydxlkFavXm3mw4YNi8zmzp1rjp0yZUpCc+qtixcvRmabNm0yx3788cdmvnz5cjOfOHGimScj2Za1SI/1mLRYLBaZJfxjvIjkAfhPAIsBTAfwqIhMT/TvI6L0SuZ39jkADqpqnapeBvBXAPenZlpElGrJFPs4AN92+/ho/LYfEJFlIlIjIjWtra1J3B0RJSPtr8ar6hpVjalqzHtBhYjSJ5libwBQ1u3j8fHbiCgHJVPs1QAmi0iFiAwA8AiAD1IzLSJKtYRbb6p6VUSeAfA/6Gq9rVXVr1I2sz7kyJEjZr5582YzLy4uNvMNGzaYeUFBQWT22WefmWO99pbXmmtrazPzQ4cORWaLFy82x1ZXV5t5Z2enmS9YsCAyq6ioMMfOnDnTzNPVOkunpPrsqvohgA9TNBciSiNeLksUCBY7USBY7ESBYLETBYLFThQIFjtRICSTu8vGYjHtq0tcP/3008isqqrKHOutKZ8wYYKZjxkzxsxfffXVyGzevHnm2JKSEjN/+umnzfz8+fNm/vrrr0dmZ8+eNcdeunTJzB966CEzr62tjcy8tfTevgtLly4182yJxWKoqanp8SIAPrMTBYLFThQIFjtRIFjsRIFgsRMFgsVOFIiMbiWdyzo6Osz866+/jswqKyvNsV7rzWMtYQWA559/PjJ76qmnzLH19fVm/sYbb5j5DTfYzxdlZWWR2eXLl82xK1asMPMLFy6YubWVtLdrkve4NDU1mbnX0rSW53qPaaL4zE4UCBY7USBY7ESBYLETBYLFThQIFjtRIFjsRIHoU332ZJbjelv/elsuW8cme9speyepjho1KuH7Buxjl7dt22aO9ZaZHj9+3Myt46IBYMCAAZHZoEGDzLHffvutmXtLYEtLSyOzq1evmmOvXLli5u+8846ZP/fcc2aerl66eZ8Zv0ciygoWO1EgWOxEgWCxEwWCxU4UCBY7USBY7ESB6FN99nQek+ttcW1tLXz33XebYw8cOGDmFy9eNHNvPbu1Nnrr1q3mWKtHDwBDhgwx81OnTpm5tabcW4/u3bf3uFm99HPnzpljveOgvT65t8W2dY2Bdz1JonWQVLGLSD2AdgAdAK6qaiyZv4+I0icVz+x3qap9mRURZR1/ZycKRLLFrgD+ISI7RGRZT58gIstEpEZEalpbW5O8OyJKVLLFPl9VbwOwGMByEbnj2k9Q1TWqGlPVmLfJHxGlT1LFrqoN8bctAN4DMCcVkyKi1Eu42EWkUESGfPc+gJ8BiD42k4iyKplX44sBvBfv+fUDsF5V/zsls0oD7/UCr2+6ffv2yGzv3r3m2Pvuu8/MX375ZTOfM8f+gWnJkiWRWUVFhTn2lVdeMfOGhgYzX7VqlZkfOXIkMvPWhHtrzh988EEzP3HiRGTmXV/g9bqbm5vNfP/+/WZ+2223RWbpup4k4WJX1ToAt6RwLkSURmy9EQWCxU4UCBY7USBY7ESBYLETBaJPLXFN5pjbw4cPm7m3bfHs2bMjM2+pprcVtLdtsXe08aJFiyKzd9991xy7Z88eM/faht4y1A0bNkRm3hLVxsZGM/eWoVotrP79+5tjve29va/7m2++MXOr9ZYufGYnCgSLnSgQLHaiQLDYiQLBYicKBIudKBAsdqJA9Kk+ezLH3Hr95MrKSjO3+vDedsr9+tkPs9dz9ZZT/uQnP4nM1q9fb44dOXKkmc+fP9/Mt2zZYub79u2LzG699VZzrLe9944dO8x85syZkdnRo0fNsUOHDjVzb3vw4uJiM88GPrMTBYLFThQIFjtRIFjsRIFgsRMFgsVOFAgWO1Eg+lSfPRlnz54188GDB5u5dQTv2LFjzbHDhg0z87a2NjM/duyYmc+bNy8y89aEe3Pzjjb2toO2rk+YPn26OTYvL8/MvbnV1dVFZkVFReZYz/jx483c+55Z22R712Ukis/sRIFgsRMFgsVOFAgWO1EgWOxEgWCxEwWCxU4UiOumz271wQF/zXl+fr6ZFxQURGbe2uZBgwaZ+cGDB818586dZr58+fLI7JNPPjHHev1irw/vrWdfuHBhZPbss8+aY9966y0zv+mmm8zc29vdUlVVZeaTJk0y8+HDh5u5dY6Bt7dCotxndhFZKyItIlLb7baRIrJRRA7E345Iy+yIKGV682P8HwHcc81tLwCoUtXJAKriHxNRDnOLXVU3Azh5zc33A1gXf38dgAdSPC8iSrFEX6ArVtXvLrpuAhC54ZaILBORGhGpaW1tTfDuiChZSb8ar6oKQI18jarGVDU2evToZO+OiBKUaLE3i0gpAMTf2seUElHWJVrsHwB4PP7+4wDeT810iChd3D67iLwNYAGAIhE5CuA3AF4D8I6IPAngMICl6Zxkb3hnoHv5LbfcYuaff/55ZObtMT5jxgwzLykpMfMFCxaYubUWv6Ojwxzr5dYZ5wDw8MMPm7n1tR8/ftwc6/WbvXMErLX0X375pTl227ZtZn7PPdc2qH5o48aNZn7y5LWvef+/dPXZ3WJX1Ucjop+meC5ElEa8XJYoECx2okCw2IkCwWInCgSLnSgQ180S1zNnzpi5t3VwQ0ODmVdUVERmK1euNMfOnj3bzJ944gkzv3LliplbS2yt45wB++vy/m4AmDZtmpmPGzcuMvOOTfbmfunSJTOfMGFCZGZt5QzYrTHAXvIM+O1Uq/Xn/XtJFJ/ZiQLBYicKBIudKBAsdqJAsNiJAsFiJwoEi50oENdNn33fvn1m7m0lPWrUKDO3lpF6vWqvx+8tt1y1apWZr169OjKrrq42x1rHGgN+P/qjjz4y87Vr10Zm3jHaXt7U1GTmAwcOjMy8r9tbfutdl1FYWGjm3rUT6cBndqJAsNiJAsFiJwoEi50oECx2okCw2IkCwWInCsR102f3jmweOXKkmbe1tZm5taXyvHnzzLHekc3eds4nTpwwc+to4mPHjpljx4wZY+beuuyZM2eaudVnnzt3rjnW2goaAKZOnWrm1uPqHUXmHVXdv39/M/eu6/D+TaQDn9mJAsFiJwoEi50oECx2okCw2IkCwWInCgSLnSgQ102f/dy5c2ZeXFxs5ocOHTJza4/yG2+80Rzr7UHurW321sNPnDgxMrv99tvNsRcvXjRzrx/s9aO3bNkSmXk9+kWLFpn55cuXzdza2927dsG7/sDb/8C7RuD06dORmXe8uDe3KO4zu4isFZEWEantdttLItIgIrvif5YkdO9ElDG9+TH+jwB6Onn+d6o6K/7nw9ROi4hSzS12Vd0MwP45lIhyXjIv0D0jIrvjP+aPiPokEVkmIjUiUuNdj0xE6ZNosb8BoBLALACNACJ3RFTVNaoaU9XY6NGjE7w7IkpWQsWuqs2q2qGqnQD+AGBOaqdFRKmWULGLSGm3D38OoDbqc4koN7h9dhF5G8ACAEUichTAbwAsEJFZABRAPYBfpnGO37POYPf6xR5rvTpg72HunROen59v5l6Pf/LkyWa+c+fOyMzqcwPAY489ZuZev9i7RqCsrCwy27Rpkzl24cKFZu5dA9Dc3ByZzZo1yxzb2Nho5t659Z2dnWY+fvz4yCxde8q7xa6qj/Zw85tpmAsRpREvlyUKBIudKBAsdqJAsNiJAsFiJwpEn1riqqoJZb0xYkTkFb8AgN27d0dmpaWlkRngt1KmTZtm5l5rbuXKlZGZt011e3u7me/fv9/MvaON+/WL/ifmta8eeeSRpO7baonefPPN5lhvWbHXqvWW31pHPntHVSeKz+xEgWCxEwWCxU4UCBY7USBY7ESBYLETBYLFThSIPtVnt3hH6A4ePNjMvb6odSzy0KFDzbHecslJkyaZ+cCBA818ypQpkZl1fQBgL48F/KONvcfNun7B61V71y8cPHjQzK1tri9cuGCO9XZVspbPAv61FdYR4t626IniMztRIFjsRIFgsRMFgsVOFAgWO1EgWOxEgWCxEwWiT/XZre2i29razLE33GD/v+ZtB231PisrK82xXs/16NGjZr5ixQozt7aq9tar19fXm/kdd9xh5uXl5WZufc/y8vLMsd5aeqtXDQDHjh2LzEpKSsyx3uPm3be3j4D1b8LbIjtRfGYnCgSLnSgQLHaiQLDYiQLBYicKBIudKBAsdqJA9Kk+++nTpyMzrzfprQn3+s3WPuJej9/r4RcUFJj5jh07zNy6hqCwsNAc6/WTvT3MvV659bUfPnzYHDt27Fgz99akW99zrw9eV1dn5k1NTWZ+1113mbl1/973JFHuM7uIlInIxyKyV0S+EpFfxW8fKSIbReRA/K19ygIRZVVvfoy/CuDXqjodwDwAy0VkOoAXAFSp6mQAVfGPiShHucWuqo2q+kX8/XYA+wCMA3A/gHXxT1sH4IF0TZKIkvdPvUAnIuUAbgXwOYBiVf1uc7UmAMURY5aJSI2I1Hj7mRFR+vS62EVkMIC/AVihqme6Z9p1qmKPJyuq6hpVjalqzNvEj4jSp1fFLiL90VXof1HVv8dvbhaR0nheCqAlPVMkolRwW2/Std/vmwD2qepvu0UfAHgcwGvxt++nZYbdWNsSe0cue1tNe1siW60Ua5tpAKitrTVzb5motxyzs7MzMjt//rw51su9x807NtnaLrq4uMff/L5nHfcM+C3N4cOHR2ZnzpyJzADgzjvvNPP169ebubW8FrCXuHrfk0T1ps9+O4BfANgjIrvit72IriJ/R0SeBHAYwNK0zJCIUsItdlX9FEDUf88/Te10iChdeLksUSBY7ESBYLETBYLFThQIFjtRIPrUEteTJ09GZlevXjXHesshva2Brav/vOWxU6dONfNkj+i1lpl6fXJvi21v6bB3VaS1lbS3zNT7nnpHZVu9bu/r8q59GDVqlJl7X5t1jUG6rjTlMztRIFjsRIFgsRMFgsVOFAgWO1EgWOxEgWCxEwWiT/XZrTXnjY2NkRkANDQ0mPm9995r5lbvc9u2beZYr4fvHensrdu2tkz21ulbfXDA79N7vXDrGgDrugkAGDx4sJl7461eundtg7cF95gxY8zc24LbelzTtZ6dz+xEgWCxEwWCxU4UCBY7USBY7ESBYLETBYLFThSIPtVnt/YgnzFjhjk2Pz/fzJPZo9zrB3vH+5aVlSU1/sSJE5FZ12E90bw+uae5udnMrZ7xsGHDzLHe4+rtWV9aWhqZbd26NeGxgH+cdHV1tZmfOnUqMvOOAE8Un9mJAsFiJwoEi50oECx2okCw2IkCwWInCgSLnSgQvTmfvQzAnwAUA1AAa1T19yLyEoB/BdAa/9QXVfXDdE0UsNd919XVmWMPHz5s5osXLzbzgoICM7d4/WSr5wr4e5xb/ejTp0+bY711/t71Cd7+6tb1CdZ1E4A/9yFDhpi51eP3HlNrjwDA/7q3b99u5tZ6eO/6gkT15qKaqwB+rapfiMgQADtEZGM8+52qvp6WmRFRSvXmfPZGAI3x99tFZB+AcemeGBGl1j/1O7uIlAO4FcDn8ZueEZHdIrJWREZEjFkmIjUiUtPa2trTpxBRBvS62EVkMIC/AVihqmcAvAGgEsAsdD3zr+ppnKquUdWYqsbSdYYVEfl6Vewi0h9dhf4XVf07AKhqs6p2qGongD8AmJO+aRJRstxil66XTN8EsE9Vf9vt9u7Lgn4OoDb10yOiVOnNq/G3A/gFgD0isit+24sAHhWRWehqx9UD+GVaZtiN1QYaP368ObalpcXMvVbKkSNHIjNveeyAAQPM3Nt22NtK2moLenPzjmz2tmsuKioyc2vLZm957fDhw83cY22jbW1x3RvTpk0zc6+taN2/t7V4onrzavynAHqaeVp76kSUWryCjigQLHaiQLDYiQLBYicKBIudKBAsdqJA9KmtpK2loF5f0+MtYbWWqZaUlJhjvW2ura2gAeDChQtmbvXSvT57RUWFmXuPq7cU1LpGwHvMveOivWsnrOsb2tvbzbEeb+nv0KFDzdzqs3d0dCQ0Jw+f2YkCwWInCgSLnSgQLHaiQLDYiQLBYicKBIudKBDiHemb0jsTaQXQfU/nIgD2ubvZk6tzy9V5AZxbolI5twmq2uP+bxkt9h/duUiNqsayNgFDrs4tV+cFcG6JytTc+GM8USBY7ESByHaxr8ny/VtydW65Oi+Ac0tURuaW1d/ZiShzsv3MTkQZwmInCkRWil1E7hGRr0XkoIi8kI05RBGRehHZIyK7RKQmy3NZKyItIlLb7baRIrJRRA7E3/Z4xl6W5vaSiDTEH7tdIrIkS3MrE5GPRWSviHwlIr+K357Vx86YV0Yet4z/zi4ieQD+F8DdAI4CqAbwqKruzehEIohIPYCYqmb9AgwRuQPAWQB/UtUZ8dv+A8BJVX0t/h/lCFX9txyZ20sAzmb7GO/4aUWl3Y8ZB/AAgH9BFh87Y15LkYHHLRvP7HMAHFTVOlW9DOCvAO7PwjxynqpuBnDtkSz3A1gXf38duv6xZFzE3HKCqjaq6hfx99sBfHfMeFYfO2NeGZGNYh8H4NtuHx9Fbp33rgD+ISI7RGRZtifTg2JVbYy/3wSgOJuT6YF7jHcmXXPMeM48dokcf54svkD3Y/NV9TYAiwEsj/+4mpO063ewXOqd9uoY70zp4Zjx72XzsUv0+PNkZaPYGwCUdft4fPy2nKCqDfG3LQDeQ+4dRd383Qm68bf2rosZlEvHePd0zDhy4LHL5vHn2Sj2agCTRaRCRAYAeATAB1mYx4+ISGH8hROISCGAnyH3jqL+AMDj8fcfB/B+FufyA7lyjHfUMePI8mOX9ePPVTXjfwAsQdcr8t8A+PdszCFiXhMBfBn/81W25wbgbXT9WHcFXa9tPAlgFIAqAAcAbAIwMofm9mcAewDsRldhlWZpbvPR9SP6bgC74n+WZPuxM+aVkceNl8sSBYIv0BEFgsVOFAgWO1EgWOxEgWCxEwWCxU4UCBY7USD+D0ARLu1wTbzDAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. What is the label for the index 500 in the test_label and looking up in the above list, what does it mean?\n",
        "\n",
        "print(test_labels[500])\n",
        "\n",
        "\n",
        "# This label for this index is 2 which using the above list represents the category 'Pullover'. As evidenced by the plotted image above\n",
        "# this is correct \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNtm_rGkc2Rk",
        "outputId": "7a9ee120-a971-48cd-b33e-3f0464fde974"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 10 Import models and layers from keras library \n",
        "from keras import models \n",
        "from keras import layers\n",
        "\n",
        "# 11 Define a sequential model and call it myNetwork \n",
        "myNetwork = models.Sequential()\n",
        "\n",
        "# 12 and 13 reshape the images from 28*28 to one column with 784 neurons (flattening) \n",
        "# Also Normalize \n",
        "\n",
        "# Flattening is the process of converting 2d image data into a 1 dimensional array to simplify the input data and make it easier to process\n",
        "\n",
        "#Book Method Flattening \n",
        "train_images = train_images.reshape((60000, 28 * 28))\n",
        "test_images = test_images.reshape((10000, 28 * 28))\n",
        "\n",
        "\n",
        "# Normalization is the scaling of pixel values to a consistent range. In our case 0-1, this improves speed and stability \n",
        "# Book Normalization\n",
        "train_images = train_images.astype('float32')/255\n",
        "test_images = test_images.astype('float32')/255\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Video Flattening\n",
        "# myNetwork.add(layers.Flatten())\n",
        "\n",
        "# Video Normalization \n",
        "# train_images = utils.normalize(train_images, axis=1)\n",
        "# test_images = utils.normalize(test_images, axis=1)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-5ZP2fL0dcIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UbtXdLkyopxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_images[0]\n",
        "# train_images[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPPc3-eIgoI4",
        "outputId": "d8e6b8c4-0ca8-4bce-da25-89e76aa0c5e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.01176471, 0.00392157, 0.        , 0.        , 0.02745098,\n",
              "       0.        , 0.14509805, 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.00392157, 0.00784314, 0.        ,\n",
              "       0.10588235, 0.32941177, 0.04313726, 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.46666667,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.00392157, 0.        , 0.        , 0.34509805, 0.56078434,\n",
              "       0.43137255, 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.08627451, 0.3647059 , 0.41568628, 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.01568628, 0.        ,\n",
              "       0.20784314, 0.5058824 , 0.47058824, 0.5764706 , 0.6862745 ,\n",
              "       0.6156863 , 0.6509804 , 0.5294118 , 0.6039216 , 0.65882355,\n",
              "       0.54901963, 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.00784314, 0.        , 0.04313726, 0.5372549 , 0.50980395,\n",
              "       0.5019608 , 0.627451  , 0.6901961 , 0.62352943, 0.654902  ,\n",
              "       0.69803923, 0.58431375, 0.5921569 , 0.5647059 , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.00392157, 0.        , 0.00784314,\n",
              "       0.00392157, 0.        , 0.01176471, 0.        , 0.        ,\n",
              "       0.4509804 , 0.44705883, 0.41568628, 0.5372549 , 0.65882355,\n",
              "       0.6       , 0.6117647 , 0.64705884, 0.654902  , 0.56078434,\n",
              "       0.6156863 , 0.61960787, 0.04313726, 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.00392157, 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.01176471,\n",
              "       0.        , 0.        , 0.34901962, 0.54509807, 0.3529412 ,\n",
              "       0.36862746, 0.6       , 0.58431375, 0.5137255 , 0.5921569 ,\n",
              "       0.6627451 , 0.6745098 , 0.56078434, 0.62352943, 0.6627451 ,\n",
              "       0.1882353 , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.00784314, 0.01568628,\n",
              "       0.00392157, 0.        , 0.        , 0.        , 0.38431373,\n",
              "       0.53333336, 0.43137255, 0.42745098, 0.43137255, 0.63529414,\n",
              "       0.5294118 , 0.5647059 , 0.58431375, 0.62352943, 0.654902  ,\n",
              "       0.5647059 , 0.61960787, 0.6627451 , 0.46666667, 0.        ,\n",
              "       0.        , 0.        , 0.00784314, 0.00784314, 0.00392157,\n",
              "       0.00784314, 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.10196079, 0.42352942, 0.45882353, 0.3882353 , 0.43529412,\n",
              "       0.45882353, 0.53333336, 0.6117647 , 0.5254902 , 0.6039216 ,\n",
              "       0.6039216 , 0.6117647 , 0.627451  , 0.5529412 , 0.5764706 ,\n",
              "       0.6117647 , 0.69803923, 0.        , 0.01176471, 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.08235294, 0.20784314, 0.36078432, 0.45882353, 0.43529412,\n",
              "       0.40392157, 0.4509804 , 0.5058824 , 0.5254902 , 0.56078434,\n",
              "       0.6039216 , 0.64705884, 0.6666667 , 0.6039216 , 0.5921569 ,\n",
              "       0.6039216 , 0.56078434, 0.5411765 , 0.5882353 , 0.64705884,\n",
              "       0.16862746, 0.        , 0.        , 0.09019608, 0.21176471,\n",
              "       0.25490198, 0.29803923, 0.33333334, 0.4627451 , 0.5019608 ,\n",
              "       0.48235294, 0.43529412, 0.44313726, 0.4627451 , 0.49803922,\n",
              "       0.49019608, 0.54509807, 0.52156866, 0.53333336, 0.627451  ,\n",
              "       0.54901963, 0.60784316, 0.6313726 , 0.5647059 , 0.60784316,\n",
              "       0.6745098 , 0.6313726 , 0.7411765 , 0.24313726, 0.        ,\n",
              "       0.26666668, 0.36862746, 0.3529412 , 0.43529412, 0.44705883,\n",
              "       0.43529412, 0.44705883, 0.4509804 , 0.49803922, 0.5294118 ,\n",
              "       0.53333336, 0.56078434, 0.49411765, 0.49803922, 0.5921569 ,\n",
              "       0.6039216 , 0.56078434, 0.5803922 , 0.49019608, 0.63529414,\n",
              "       0.63529414, 0.5647059 , 0.5411765 , 0.6       , 0.63529414,\n",
              "       0.76862746, 0.22745098, 0.27450982, 0.6627451 , 0.5058824 ,\n",
              "       0.40784314, 0.38431373, 0.39215687, 0.36862746, 0.38039216,\n",
              "       0.38431373, 0.4       , 0.42352942, 0.41568628, 0.46666667,\n",
              "       0.47058824, 0.5058824 , 0.58431375, 0.6117647 , 0.654902  ,\n",
              "       0.74509805, 0.74509805, 0.76862746, 0.7764706 , 0.7764706 ,\n",
              "       0.73333335, 0.77254903, 0.7411765 , 0.72156864, 0.14117648,\n",
              "       0.0627451 , 0.49411765, 0.67058825, 0.7372549 , 0.7372549 ,\n",
              "       0.72156864, 0.67058825, 0.6       , 0.5294118 , 0.47058824,\n",
              "       0.49411765, 0.49803922, 0.57254905, 0.7254902 , 0.7647059 ,\n",
              "       0.81960785, 0.8156863 , 1.        , 0.81960785, 0.69411767,\n",
              "       0.9607843 , 0.9882353 , 0.9843137 , 0.9843137 , 0.96862745,\n",
              "       0.8627451 , 0.80784315, 0.19215687, 0.        , 0.        ,\n",
              "       0.        , 0.04705882, 0.2627451 , 0.41568628, 0.6431373 ,\n",
              "       0.7254902 , 0.78039217, 0.8235294 , 0.827451  , 0.8235294 ,\n",
              "       0.8156863 , 0.74509805, 0.5882353 , 0.32156864, 0.03137255,\n",
              "       0.        , 0.        , 0.        , 0.69803923, 0.8156863 ,\n",
              "       0.7372549 , 0.6862745 , 0.63529414, 0.61960787, 0.5921569 ,\n",
              "       0.04313726, 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        ], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 14. Continuing from the previous question, to add one hidden layer that has 512 neurons, using ‘relu’ activation function:\n",
        "myNetwork.add(layers.Dense(512, activation='relu'))\n",
        "\n",
        "# 15. Add another hidden layer that has 128 neurons, using ‘relu’ activation function.\n",
        "myNetwork.add(layers.Dense(128, activation='relu'))\n",
        "\n",
        "# 16. Add the last layer as a 10-neuron dense layer that uses the ‘softmax’ as the activation function. Why we use softmax for the last layer? How does it work under the hood?\n",
        "myNetwork.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "# 17.\tUse the following two settings for the compiler and run them separately and see what the differences are. \n",
        "# •\tOptimizer   adam,  loss 'sparse_categorical_crossentropy', metrics[‘accuracy’]\n",
        "# •\tOptimizer   rmsprop,  loss 'categorical_crossentropy', metrics[‘accuracy’]\n",
        "\n",
        "# Setting 1\n",
        "myNetwork.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "# Setting 2\n",
        "# myNetwork.compile( optimizer='rmsprop',\n",
        "#                   loss='categorical_crossentropy',\n",
        "#                   metrics=['accuracy'])\n",
        "\n",
        "# 18. Now after the compilation, please try to find the pattern using the fit command. The epochs need to be 10 for this example.\n",
        "myNetwork.fit(train_images, train_labels, epochs=10)\n",
        "#Results Setting 1\n",
        "# Epoch 1/10\n",
        "# 1875/1875 [==============================] - 19s 10ms/step - loss: 0.4714 - accuracy: 0.8307\n",
        "# Epoch 2/10\n",
        "# 1875/1875 [==============================] - 19s 10ms/step - loss: 0.3553 - accuracy: 0.8691\n",
        "# Epoch 3/10\n",
        "# 1875/1875 [==============================] - 17s 9ms/step - loss: 0.3215 - accuracy: 0.8819\n",
        "# Epoch 4/10\n",
        "# 1875/1875 [==============================] - 17s 9ms/step - loss: 0.2976 - accuracy: 0.8892\n",
        "# Epoch 5/10\n",
        "# 1875/1875 [==============================] - 17s 9ms/step - loss: 0.2780 - accuracy: 0.8960\n",
        "# Epoch 6/10\n",
        "# 1875/1875 [==============================] - 16s 9ms/step - loss: 0.2670 - accuracy: 0.8995\n",
        "# Epoch 7/10\n",
        "# 1875/1875 [==============================] - 17s 9ms/step - loss: 0.2520 - accuracy: 0.9052\n",
        "# Epoch 8/10\n",
        "# 1875/1875 [==============================] - 18s 10ms/step - loss: 0.2417 - accuracy: 0.9096\n",
        "# Epoch 9/10\n",
        "# 1875/1875 [==============================] - 17s 9ms/step - loss: 0.2324 - accuracy: 0.9110\n",
        "# Epoch 10/10\n",
        "# 1875/1875 [==============================] - 17s 9ms/step - loss: 0.2223 - accuracy: 0.9153\n",
        "# <keras.callbacks.History at 0x7f4231441640>\n",
        "\n",
        "#Results Setting 2\n",
        "\n",
        "# Epoch 1/10\n",
        "# 1875/1875 [==============================] - 17s 9ms/step - loss: 0.4934 - accuracy: 0.8214\n",
        "# Epoch 2/10\n",
        "# 1875/1875 [==============================] - 17s 9ms/step - loss: 0.3806 - accuracy: 0.8620\n",
        "# Epoch 3/10\n",
        "# 1875/1875 [==============================] - 16s 9ms/step - loss: 0.3569 - accuracy: 0.8744\n",
        "# Epoch 4/10\n",
        "# 1875/1875 [==============================] - 16s 9ms/step - loss: 0.3432 - accuracy: 0.8796\n",
        "# Epoch 5/10\n",
        "# 1875/1875 [==============================] - 16s 8ms/step - loss: 0.3350 - accuracy: 0.8816\n",
        "# Epoch 6/10\n",
        "# 1875/1875 [==============================] - 16s 8ms/step - loss: 0.3287 - accuracy: 0.8866\n",
        "# Epoch 7/10\n",
        "# 1875/1875 [==============================] - 16s 8ms/step - loss: 0.3203 - accuracy: 0.8887\n",
        "# Epoch 8/10\n",
        "# 1875/1875 [==============================] - 16s 9ms/step - loss: 0.3164 - accuracy: 0.8902\n",
        "# Epoch 9/10\n",
        "# 1875/1875 [==============================] - 16s 8ms/step - loss: 0.3126 - accuracy: 0.8939\n",
        "# Epoch 10/10\n",
        "# 1875/1875 [==============================] - 16s 8ms/step - loss: 0.3058 - accuracy: 0.8941\n",
        "# <keras.callbacks.History at 0x7f423189a070>\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "i86FLLFqoIrT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3043b59f-896f-4598-fdcf-aac7ab30715c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 20s 9ms/step - loss: 0.4735 - accuracy: 0.8285\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 15s 8ms/step - loss: 0.3574 - accuracy: 0.8678\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 12s 7ms/step - loss: 0.3210 - accuracy: 0.8816\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.2968 - accuracy: 0.8901\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.2779 - accuracy: 0.8961\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 14s 7ms/step - loss: 0.2632 - accuracy: 0.9010\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 14s 7ms/step - loss: 0.2500 - accuracy: 0.9055\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.2383 - accuracy: 0.9099\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.2279 - accuracy: 0.9139\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.2183 - accuracy: 0.9167\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7cb20d6880>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 19 How do you compare the Fashion-MNIST with what we learned in the class using the MNIST? What can we infer from the differences in the accuracy? What could be the reasons for that?\n",
        "\n",
        "# Fashion-MNIST is similar to MNIST as they contain images within their dataset. Fashion-MNIST however is considered to present a greater challenge as the images are more complex in structure than the digit images in MNIST. For this reason the achieved accuracy \n",
        "# in comparison to MNIST is lower. "
      ],
      "metadata": {
        "id": "m7bL1XLX4_OX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 20.  20.\tUse the evaluate to calculate the achieved accuracy and loss over the test images and labels. Do we have overfitting? \n",
        "\n",
        "# I would say that we do not have overfitting as there is not a huge difference between the test and training accuracy as well as loss. \n",
        "\n",
        "test_loss, test_acc = myNetwork.evaluate(test_images, test_labels)\n",
        "\n",
        "print('Test loss:', test_loss)\n",
        "print('Test accuracy:', test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GjK7EUkk7FRg",
        "outputId": "2244728e-7685-4370-9d2b-bb2c7cc51e67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 0.3419 - accuracy: 0.8843\n",
            "Test loss: 0.34190642833709717\n",
            "Test accuracy: 0.8842999935150146\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "yncpO0UphGE3"
      }
    }
  ]
}